{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_dir = \"./data/ml-20m/pro_sg/\"\n",
    "\n",
    "unique_sid = list()\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te\n",
    "\n",
    "test_data_tr, test_data_te = load_tr_te_data(\n",
    "    os.path.join(pro_dir, 'test_tr.csv'),\n",
    "    os.path.join(pro_dir, 'test_te.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users = np.load(\"./data/movielens20M_users_svd.npy\")\n",
    "# users /= np.linalg.norm(users, axis=1, keepdims=True)\n",
    "\n",
    "# vals = np.load(\"./data/movielens20M_singular_vals_svd.npy\")\n",
    "movies = np.load(\"./data/movielens20M_users_svd.npy\")\n",
    "# movies *= np.sqrt(vals)\n",
    "# movies = np.load(\"./data/movielens20M_movies_pic_pmi.npy\")\n",
    "# movies /= np.linalg.norm(movies, axis=1, keepdims=True)\n",
    "# movies[np.isnan(movies)] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i.lobov/research/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pop = np.load(\"./data/movielens20M_movies_pop.npy\")\n",
    "scaling = 1.0 / pop**0.5\n",
    "scaling[scaling == np.inf] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop = np.asarray(test_data_tr.sum(axis=0)).flatten()\n",
    "# scaling = 1.0 / np.log(2+pop)\n",
    "users = test_data_tr.dot(movies)\n",
    "users = np.hstack([users, np.zeros([users.shape[0], 1])])\n",
    "users /= np.linalg.norm(users, axis=1, keepdims=True)\n",
    "# users = test_data_tr.dot(movies)\n",
    "\n",
    "movies /= np.linalg.norm(movies, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "Test NDCG@100=0.15412 (0.00226)\n",
      "Test Recall@20=0.09164 (0.00377)\n",
      "Test Recall@50=0.25577 (0.00571)\n",
      "1.0\n",
      "0\n",
      "Test NDCG@100=0.15708 (0.00225)\n",
      "Test Recall@20=0.09395 (0.00383)\n",
      "Test Recall@50=0.26040 (0.00572)\n",
      "3.0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-4340cdf69c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#         scaling = np.abs(vals)**0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#         pred_val = (X_users * scaling).dot((movies * scaling).T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies_pop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# exclude examples from training and validation (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for f in [0.0, 1.0, 3.0, 5.0]:\n",
    "    print(f)\n",
    "    movies_pop = np.hstack([movies, f * scaling.reshape(-1, 1)])\n",
    "    \n",
    "    # N_test = test_data_tr.shape[0]\n",
    "    N_test = 2000\n",
    "    idxlist_test = range(N_test)\n",
    "    \n",
    "    batch_size_test = 2000\n",
    "    holdout_size = 10000\n",
    "    shift = len(users)-holdout_size\n",
    "\n",
    "    n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "    for bnum, st_idx in enumerate(range(0, N_test, batch_size_test)):\n",
    "        print(bnum)\n",
    "        end_idx = min(st_idx + batch_size_test, N_test)\n",
    "        X = test_data_tr[idxlist_test[st_idx:end_idx]]\n",
    "\n",
    "        if sparse.isspmatrix(X):\n",
    "            X = X.toarray()\n",
    "        X = X.astype('float32')\n",
    "\n",
    "        X_users = users[shift+st_idx:shift+end_idx]\n",
    "    #     pred_val = X_users.dot((movies).T)\n",
    "    #         scaling = np.abs(vals)**0.0\n",
    "    #         pred_val = (X_users * scaling).dot((movies * scaling).T)\n",
    "        pred_val = 1.0 / euclidean_distances(X_users, movies_pop)\n",
    "\n",
    "        # exclude examples from training and validation (if any)\n",
    "#         pred_val[X.nonzero()] = -np.inf\n",
    "        n100_list.append(NDCG_binary_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
    "        r20_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
    "        r50_list.append(Recall_at_k_batch(pred_val, test_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
    "\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "    print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "    print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best: MovieLens 20M, PMI on movies, positive=30, 100 dims, unnormalized movies and users\n",
    "#     popularity (1/log(2+pop)) scaling for users, cds=0.75, right singular vectors\n",
    "# \n",
    "# Test NDCG@100=0.28199 (0.00407)\n",
    "# Test Recall@20=0.23678 (0.00475)\n",
    "# Test Recall@50=0.33990 (0.00551)\n",
    "\n",
    "\n",
    "# PIC, iters=3, unnormalized movies, users = sum of movies, cds=0.75\n",
    "# rng = np.random.RandomState(0)\n",
    "# U = rng.normal(size=[pmi.shape[0], dim])\n",
    "# U = pmi.T.dot(U)\n",
    "# U, _ = np.linalg.qr(U)\n",
    "\n",
    "# for _ in range(3):\n",
    "#     U = pmi.dot(U)\n",
    "#     U, _ = np.linalg.qr(U)\n",
    "    \n",
    "#     U = pmi.T.dot(U)\n",
    "#     U, _ = np.linalg.qr(U)\n",
    "\n",
    "# Test NDCG@100=0.28099 (0.00405)\n",
    "# Test Recall@20=0.23463 (0.00474)\n",
    "# Test Recall@50=0.33948 (0.00549)\n",
    "\n",
    "# Iterations of PIC\n",
    "# 1\n",
    "# Test NDCG@100=0.22021 (0.00347)\n",
    "# Test Recall@20=0.17242 (0.00410)\n",
    "# Test Recall@50=0.27906 (0.00514)\n",
    "# 2\n",
    "# Test NDCG@100=0.26983 (0.00390)\n",
    "# Test Recall@20=0.22301 (0.00462)\n",
    "# Test Recall@50=0.32750 (0.00534)\n",
    "# 3\n",
    "# Test NDCG@100=0.27299 (0.00401)\n",
    "# Test Recall@20=0.22580 (0.00470)\n",
    "# Test Recall@50=0.33259 (0.00544)\n",
    "# 4\n",
    "# Test NDCG@100=0.27602 (0.00402)\n",
    "# Test Recall@20=0.22859 (0.00466)\n",
    "# Test Recall@50=0.33790 (0.00547)\n",
    "# 5\n",
    "# Test NDCG@100=0.27313 (0.00399)\n",
    "# Test Recall@20=0.22673 (0.00465)\n",
    "# Test Recall@50=0.33048 (0.00544)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (research)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
